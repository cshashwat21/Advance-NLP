{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7d53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cdae873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407a450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "#using distillbert\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723a47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(r\"/Users/chandanshashwat/Desktop/DataSet/twitterS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e27eef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_original[:5000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a282fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                               Text\n",
       "0  Positive  im getting on borderlands and i will murder yo...\n",
       "1  Positive  I am coming to the borders and I will kill you...\n",
       "2  Positive  im getting on borderlands and i will kill you ...\n",
       "3  Positive  im coming on borderlands and i will murder you...\n",
       "4  Positive  im getting on borderlands 2 and i will murder ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f8ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "023b8505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc8e1746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative', 'Irrelevant'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57754354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive      1938\n",
       "Neutral       1166\n",
       "Negative      1056\n",
       "Irrelevant     840\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47912418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment']= df['Sentiment'].replace(['Positive', 'Neutral', 'Negative', 'Irrelevant'], [0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82bb2fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20cc1c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment    0\n",
       "Text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7665653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(df[\"Text\"].values)\n",
    "y = list(df[\"Sentiment\"].values)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    X, y, random_state=42, test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c20c67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cd2dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hamspamdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = hamspamdataset(train_encodings, train_labels)\n",
    "test_dataset = hamspamdataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6409969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return {\n",
    "#         'accuracy': acc,\n",
    "#         'f1': f1,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64a0b46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file config.json from cache at /Users/chandanshashwat/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/chandanshashwat/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",  # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # total number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    warmup_steps=1000,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir=\"logs\",  # directory for storing logs\n",
    "    logging_steps=5000,  # default: 500\n",
    "    save_steps=5000,  # default: 500\n",
    "    learning_rate=1e-5,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    seed=16,\n",
    "    gradient_accumulation_steps=8,  # reduce memory usage while allowing bigger overall batch size.\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # the instantiated Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "#     compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,  # training dataset\n",
    "    eval_dataset=test_dataset,  # test dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26d3dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3961\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 369\n",
      "  Number of trainable parameters = 66956548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='369' max='369' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [369/369 37:44, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=369, training_loss=1.3326665780085536, metrics={'train_runtime': 2269.9504, 'train_samples_per_second': 5.235, 'train_steps_per_second': 0.163, 'total_flos': 699521117448384.0, 'train_loss': 1.3326665780085536, 'epoch': 2.99})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16dd32cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1703726053237915,\n",
       " 'eval_runtime': 44.2624,\n",
       " 'eval_samples_per_second': 22.389,\n",
       " 'eval_steps_per_second': 5.603,\n",
       " 'epoch': 2.99}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f693fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to finetuned/twitter_sentim\n",
      "Configuration saved in finetuned/twitter_sentim/config.json\n",
      "Model weights saved in finetuned/twitter_sentim/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "ft_model = \"finetuned/twitter_sentim\"\n",
    "trainer.save_model(ft_model)\n",
    "# tokenizer.save_pretrained(ft_model)\n",
    "# output_dir = './model_save/'\n",
    "# model_to_save.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1cac76c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in finetuned/tokenizer/tokenizer_config.json\n",
      "Special tokens file saved in finetuned/tokenizer/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('finetuned/tokenizer/tokenizer_config.json',\n",
       " 'finetuned/tokenizer/special_tokens_map.json',\n",
       " 'finetuned/tokenizer/vocab.txt',\n",
       " 'finetuned/tokenizer/added_tokens.json',\n",
       " 'finetuned/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_tokanizer = \"finetuned/tokenizer\"\n",
    "tokenizer.save_pretrained(ft_tokanizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "648aeaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 9212, 7847, 3527, 3527, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"chandandodo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16dba4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 991\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.92060155,  0.01549772, -0.48461488, -0.48578054],\n",
       "       [ 0.773078  ,  0.08366629, -0.40965343, -0.38616055],\n",
       "       [ 0.5110557 , -0.225641  , -0.07265653, -0.5824527 ],\n",
       "       ...,\n",
       "       [ 0.78424215, -0.30756932, -0.53917176, -0.20832232],\n",
       "       [ 0.6231303 ,  0.12010843, -0.56717354, -0.115559  ],\n",
       "       [ 0.91952723, -0.19310656, -0.5575187 , -0.49310625]],\n",
       "      dtype=float32), label_ids=array([0, 0, 2, 0, 0, 2, 1, 2, 2, 0, 1, 0, 0, 1, 1, 1, 3, 1, 0, 2, 2, 1,\n",
       "       3, 2, 0, 3, 1, 1, 0, 1, 1, 2, 1, 1, 1, 3, 0, 3, 3, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 3, 1, 2, 0, 1, 2, 2, 3, 3, 0, 1, 2, 0, 0, 0, 1, 0, 2, 0,\n",
       "       1, 0, 0, 2, 3, 1, 3, 3, 0, 0, 0, 0, 3, 0, 2, 0, 0, 2, 1, 0, 0, 3,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 1, 1,\n",
       "       2, 0, 0, 3, 3, 0, 3, 1, 1, 3, 0, 0, 2, 3, 2, 0, 1, 1, 1, 0, 2, 0,\n",
       "       0, 0, 1, 1, 0, 3, 0, 0, 0, 0, 0, 2, 0, 1, 3, 3, 0, 2, 0, 2, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 3, 0, 0, 0, 0, 0, 1, 3, 0, 2, 0, 0, 0, 2, 1,\n",
       "       0, 3, 2, 1, 0, 3, 1, 2, 1, 3, 2, 0, 2, 0, 0, 0, 1, 0, 2, 0, 1, 3,\n",
       "       0, 2, 0, 0, 0, 2, 2, 0, 2, 1, 2, 2, 1, 0, 0, 3, 1, 3, 2, 0, 0, 0,\n",
       "       0, 1, 3, 3, 0, 3, 0, 0, 2, 2, 0, 0, 0, 1, 0, 3, 2, 2, 3, 2, 2, 0,\n",
       "       0, 2, 0, 0, 2, 3, 0, 0, 0, 0, 0, 2, 2, 1, 2, 0, 3, 3, 0, 0, 3, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 2, 0, 3, 0, 2, 0, 2, 3, 2, 0, 1, 3, 1, 3,\n",
       "       0, 2, 3, 2, 1, 0, 1, 1, 0, 1, 2, 2, 3, 0, 3, 2, 1, 3, 2, 0, 3, 1,\n",
       "       2, 1, 0, 1, 0, 1, 0, 1, 1, 1, 2, 0, 2, 0, 3, 0, 2, 1, 1, 0, 1, 1,\n",
       "       1, 2, 2, 2, 3, 0, 0, 0, 1, 2, 2, 2, 0, 1, 3, 2, 0, 0, 2, 1, 2, 1,\n",
       "       1, 1, 0, 2, 3, 2, 0, 2, 1, 1, 0, 3, 0, 2, 1, 1, 3, 3, 3, 3, 3, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 3, 3, 2, 1, 2, 0, 2, 1, 0, 1, 1, 3,\n",
       "       0, 2, 1, 0, 1, 1, 2, 0, 3, 2, 0, 0, 0, 0, 0, 2, 0, 2, 3, 1, 1, 3,\n",
       "       0, 1, 2, 1, 0, 0, 1, 1, 1, 1, 2, 1, 2, 1, 0, 2, 2, 1, 3, 1, 0, 0,\n",
       "       2, 3, 1, 0, 1, 2, 2, 0, 0, 3, 3, 0, 0, 1, 0, 3, 1, 0, 1, 1, 2, 2,\n",
       "       0, 1, 2, 0, 3, 0, 2, 0, 2, 2, 1, 3, 0, 1, 0, 0, 3, 1, 0, 2, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 0, 3, 1, 2, 3, 1, 0, 2, 2, 2,\n",
       "       0, 0, 0, 1, 2, 2, 1, 2, 3, 3, 1, 1, 0, 0, 0, 2, 1, 2, 3, 1, 3, 0,\n",
       "       1, 1, 0, 3, 0, 3, 0, 1, 3, 0, 2, 3, 2, 1, 0, 0, 3, 1, 2, 0, 3, 3,\n",
       "       2, 1, 1, 1, 1, 2, 3, 0, 1, 2, 1, 0, 3, 0, 0, 0, 2, 3, 1, 0, 2, 2,\n",
       "       3, 2, 2, 0, 0, 0, 1, 0, 1, 0, 2, 3, 2, 2, 1, 2, 1, 0, 1, 0, 3, 0,\n",
       "       2, 3, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 1, 3, 1, 0, 0, 1, 3,\n",
       "       1, 1, 0, 3, 3, 0, 0, 1, 3, 0, 0, 0, 1, 3, 2, 2, 3, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 2, 2, 0, 3, 1, 2, 0, 3,\n",
       "       0, 3, 0, 0, 0, 3, 0, 3, 0, 0, 2, 2, 0, 0, 0, 0, 0, 3, 1, 0, 0, 3,\n",
       "       3, 2, 3, 3, 1, 1, 1, 0, 1, 2, 2, 0, 0, 0, 3, 1, 0, 0, 1, 2, 0, 0,\n",
       "       3, 2, 3, 3, 3, 1, 2, 1, 0, 0, 0, 2, 0, 1, 2, 1, 3, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 2, 0, 2, 0, 1, 0, 0, 3, 1, 0, 3, 3, 1, 3, 0, 3, 1, 0, 2,\n",
       "       1, 0, 0, 3, 1, 0, 1, 2, 2, 2, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 2, 0, 1, 2, 3, 0, 3, 2, 0, 1, 1, 0, 0, 3, 0, 3, 0, 2, 0, 1, 2,\n",
       "       0, 1, 3, 1, 0, 0, 0, 2, 1, 2, 1, 1, 3, 2, 3, 0, 0, 0, 0, 0, 2, 1,\n",
       "       2, 2, 3, 3, 3, 1, 3, 0, 1, 0, 1, 2, 1, 1, 0, 3, 2, 2, 0, 3, 0, 2,\n",
       "       0, 2, 0, 2, 2, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 1, 0, 2, 1, 1,\n",
       "       1, 0, 0, 0, 0, 2, 1, 0, 2, 0, 3, 2, 1, 0, 0, 1, 1, 0, 0, 0, 3, 2,\n",
       "       2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 3, 3, 0, 0, 1, 0, 0, 0, 3, 1, 1,\n",
       "       1, 0, 0, 0, 2, 0, 0, 1, 2, 1, 3, 0, 1, 3, 3, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 2, 0, 3, 0, 2, 3, 3, 3, 1, 2, 1, 0, 0, 2, 2, 1, 2, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 1, 1, 1, 3, 2, 3, 0, 1, 3, 3, 3,\n",
       "       3, 1, 1, 1, 3, 0, 3, 3, 1, 2, 2, 3, 1, 0, 2, 3, 2, 1, 3, 0, 0, 0,\n",
       "       0]), metrics={'test_loss': 1.1703726053237915, 'test_runtime': 43.2443, 'test_samples_per_second': 22.916, 'test_steps_per_second': 5.735})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction \n",
    "trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15d32587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./finetuned/twitter_sentim/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./finetuned/twitter_sentim/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./finetuned/twitter_sentim/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_dist = DistilBertForSequenceClassification.from_pretrained('./finetuned/twitter_sentim/',local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "283dc896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#     logits = model_dist(**inputs).logits\n",
    "\n",
    "# inputs = tokenizer(df_train[0], return_tensors=\"pt\")\n",
    "\n",
    "# predicted_class_id = logits.argmax().item()\n",
    "# model.config.id2label[predicted_class_id]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "text = [\"no\"]\n",
    "encoding = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "outputs = model_dist(**encoding)\n",
    "predictions = outputs.logits.argmax(-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "54885831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.39412593841552734},\n",
       "  {'label': 'LABEL_1', 'score': 0.1777602583169937},\n",
       "  {'label': 'LABEL_2', 'score': 0.25337693095207214},\n",
       "  {'label': 'LABEL_3', 'score': 0.17473682761192322}]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs_prediction = tokenizer(df_train[0], return_tensors=\"pt\")\n",
    "# outputs = model_dist(**inputs_prediction)\n",
    "\n",
    "# logits = outputs.logits\n",
    "from transformers import TextClassificationPipeline\n",
    "pipe = TextClassificationPipeline(model=model_dist, tokenizer=tokenizer)\n",
    "prediction = pipe(\"The text to predict\", return_all_scores=True)\n",
    "prediction\n",
    "#check in config.json file the Labels defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "predicted_label_classes = logits.argmax(-1)\n",
    "print(predicted_label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40baf13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a0a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48ee2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
